{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Determine best number of epochs from early stopping\n",
        "best_epoch_count = early_stopping.stopped_epoch - early_stopping.patience + 1\n",
        "print(f'Retraining on full dataset for {best_epoch_count} epochs...')\n",
        "\n",
        "# Retrain on full dataset\n",
        "final_model = create_model()\n",
        "final_model.fit(X, y, epochs=best_epoch_count, batch_size=32, verbose=0)\n",
        "\n",
        "# Predictions\n",
        "predictions = final_model.predict(X_test_submission)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission_df = pd.DataFrame({\n",
        "    'PassengerId': test_df['PassengerId'],\n",
        "    'Survived': predicted_classes.flatten()\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file saved successfully as 'submission.csv'\")\n",
        "print(submission_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Submission Preparation\n",
        "\n",
        "Train on the full dataset for the optimal number of epochs (found during validation) and generate predictions for the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "val_loss = history.history['val_loss']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "accuracy = history.history['accuracy']\n",
        "\n",
        "# Plot Loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss, label='Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracy, label='Accuracy')\n",
        "plt.plot(val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot for README\n",
        "if not os.path.exists('images'):\n",
        "    os.makedirs('images')\n",
        "plt.savefig('images/loss_plot.png')\n",
        "plt.show()\n",
        "\n",
        "best_epoch = np.argmax(val_accuracy)\n",
        "print(f'Best Validation Accuracy: {max(val_accuracy):.4f} at epoch {best_epoch}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation and Visualization\n",
        "\n",
        "Plotting the training history (Loss and Accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train model\n",
        "model = create_model()\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                  patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=150, # Increased epochs slightly, early stopping will handle it\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training\n",
        "\n",
        "We split the data into training and validation sets (80/20).\n",
        "We use Early Stopping to prevent overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(6,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')  # Binary output\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Definition\n",
        "\n",
        "We define a sequential Keras model with:\n",
        "- Input layer (6 features)\n",
        "- Dense layers with ReLU activation and Dropout for regularization\n",
        "- Output layer with Sigmoid activation (for binary classification)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing function\n",
        "def preprocess_data(df, is_training=True):\n",
        "    features = df[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Sex']].copy()\n",
        "    \n",
        "    # Fill missing Age with median\n",
        "    features['Age'] = features['Age'].fillna(features['Age'].median())\n",
        "    \n",
        "    # Handle missing Fare in test set if any\n",
        "    features['Fare'] = features['Fare'].fillna(features['Fare'].median())\n",
        "\n",
        "    # Normalize features to <-1, 1>\n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "    features[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']] = scaler.fit_transform(features[['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']])\n",
        "\n",
        "    # Encode Sex category: 'female' -> -1, 'male' -> 1\n",
        "    label_encoder = LabelEncoder()\n",
        "    features['Sex'] = label_encoder.fit_transform(features['Sex']) * 2 - 1 \n",
        "\n",
        "    if is_training:\n",
        "        labels = df['Survived'].copy()\n",
        "        return features, labels\n",
        "    else:\n",
        "        return features\n",
        "\n",
        "# Process the data\n",
        "features, labels = preprocess_data(train_df, is_training=True)\n",
        "features_test = preprocess_data(test_df, is_training=False)\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "X = features.to_numpy()\n",
        "y = labels.to_numpy()\n",
        "X_test_submission = features_test.to_numpy().astype(float32)\n",
        "\n",
        "print(\"Data preprocessed and converted to NumPy arrays.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing\n",
        "\n",
        "We need to predict survival.\n",
        "We fill missing Age values with the median.\n",
        "Features are normalized to the range <-1, 1>.\n",
        "Sex is encoded as -1 (female) and 1 (male)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Import data\n",
        "def load_data():\n",
        "    # Adjust paths if necessary based on your environment\n",
        "    # Kaggle environment usually puts data in /kaggle/input/\n",
        "    # Local environment might differ. \n",
        "    # Using the path downloaded by kagglehub if available, or fallback to local\n",
        "    \n",
        "    possible_paths = [\n",
        "        '/kaggle/input/titanic-dataset/train.csv',\n",
        "        'train.csv', # Local fallback\n",
        "    ]\n",
        "    \n",
        "    train_path = None\n",
        "    for p in possible_paths:\n",
        "        if os.path.exists(p):\n",
        "            train_path = p\n",
        "            break\n",
        "            \n",
        "    # If using kagglehub, the path might be in the cache\n",
        "    if train_path is None:\n",
        "        try:\n",
        "            # Try to find where kagglehub put it if run previously\n",
        "            import kagglehub\n",
        "            path = kagglehub.dataset_download('wiktorabka/titanic-dataset')\n",
        "            train_path = os.path.join(path, 'train.csv')\n",
        "            test_path = os.path.join(path, 'test.csv')\n",
        "            return pd.read_csv(train_path), pd.read_csv(test_path)\n",
        "        except:\n",
        "             pass\n",
        "\n",
        "    # Hardcoded fallback for the notebook context\n",
        "    if train_path is None: \n",
        "         # Assuming standard kaggle paths for now as in original code\n",
        "         train_df = pd.read_csv('/kaggle/input/titanic-dataset/train.csv')\n",
        "         test_df = pd.read_csv('/kaggle/input/titanic-dataset/test.csv')\n",
        "         return train_df, test_df\n",
        "         \n",
        "    return pd.read_csv(train_path), pd.read_csv(test_path.replace('train.csv', 'test.csv'))\n",
        "\n",
        "try:\n",
        "    train_df, test_df = load_data()\n",
        "    print(\"Data loaded successfully\")\n",
        "    print(train_df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    print(\"Make sure to run the kagglehub download cell above or place train.csv/test.csv in the directory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n",
        "\n",
        "Load the training and test datasets.\n",
        "Ensure you have the data downloaded (via kagglehub or manually placed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import float32\n",
        "import os\n",
        "\n",
        "print(\"Imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fortland2018/ML-Projects/blob/main/Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Titanic - Machine Learning from Disaster\n",
        "\n",
        "This notebook demonstrates a solution to the classic Titanic survival prediction problem.\n",
        "We will use a Neural Network to predict passenger survival based on features like Age, Sex, Ticket Class, and Fare.\n",
        "\n",
        "## 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QBmmzaqurzD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Titanic",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 9557130,
          "datasetId": 5673692,
          "sourceId": 9358338,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30761,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
