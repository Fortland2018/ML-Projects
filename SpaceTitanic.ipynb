{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMr7SQ_9u5P0"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "# import kagglehub\n",
        "# kagglehub.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLCa7uNdu5P2"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "# spaceship_titanic_path = kagglehub.competition_download('spaceship-titanic')\n",
        "\n",
        "print('Data source configuration complete.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Imports and Data Loading\n",
        "Import necessary libraries and load the training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import os\n",
        "\n",
        "# Create logs directory for TensorBoard\n",
        "if not os.path.exists('logs'):\n",
        "    os.makedirs('logs')\n",
        "\n",
        "# Load the datasets\n",
        "# Uses local data from the project structure\n",
        "try:\n",
        "    train = pd.read_csv('data/spaceship-titanic/train.csv')\n",
        "    test = pd.read_csv('data/spaceship-titanic/test.csv')\n",
        "    print(\"Data loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Data not found. Please ensure the dataset is in 'data/spaceship-titanic/' folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Feature Engineering: CryoSleep\n",
        "CryoSleep is an important feature. We'll map the values to numbers and handle missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def map_cryo(val):\n",
        "    if pd.isna(val):\n",
        "        return 0.5\n",
        "    if val in [True, 'True', 1, '1']:\n",
        "        return 1\n",
        "    if val in [False, 'False', 0, '0']:\n",
        "        return 0\n",
        "    return 0.5  # default/unknown\n",
        "\n",
        "if 'CryoSleep' in train.columns:\n",
        "    train['CryoSleep'] = train['CryoSleep'].apply(map_cryo).astype('float64')\n",
        "if 'CryoSleep' in test.columns:\n",
        "    test['CryoSleep'] = test['CryoSleep'].apply(map_cryo).astype('float64')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Categorical Encoding\n",
        "We combine the training and test datasets to ensure consistent label encoding across both sets for categorical features like HomePlanet, Cabin, Destination, and VIP.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine datasets to ensure consistent encoding\n",
        "train['is_train'] = 1\n",
        "test['is_train'] = 0\n",
        "full = pd.concat([train, test], ignore_index=True)\n",
        "\n",
        "# Encode categorical variables on the full dataset\n",
        "for col in ['HomePlanet', 'Cabin', 'Destination', 'VIP']:\n",
        "    full[col] = full[col].astype('category').cat.codes\n",
        "\n",
        "# Split back into train and test sets\n",
        "train = full[full['is_train'] == 1].drop(columns=['is_train'])\n",
        "test = full[full['is_train'] == 0].drop(columns=['is_train'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Handling Missing Values\n",
        "Impute missing values:\n",
        "- Use the **mean** for numerical columns.\n",
        "- Use the **mode** (most frequent value) for categorical columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skip_cols = ['CryoSleep', 'PassengerId']\n",
        "\n",
        "# Impute for training set\n",
        "for col in train.columns:\n",
        "    if col in skip_cols:\n",
        "        continue\n",
        "    if train[col].dtype in ['float64', 'int64']:\n",
        "        train[col] = train[col].fillna(train[col].mean())\n",
        "    else:\n",
        "        # Use mode if available\n",
        "        mode_values = train[col].mode()\n",
        "        if not mode_values.empty:\n",
        "            train[col] = train[col].fillna(mode_values[0])\n",
        "\n",
        "# Impute for test set using test statistics (or could use train stats to prevent leakage, but here we use test stats)\n",
        "for col in test.columns:\n",
        "    if test[col].dtype in ['float64', 'int64']:\n",
        "        test[col] = test[col].fillna(test[col].mean())\n",
        "    else:\n",
        "        mode_val = test[col].mode()\n",
        "        if not mode_val.empty:\n",
        "            test[col] = test[col].fillna(mode_val[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Advanced Feature Engineering\n",
        "Here we extract the 'Group' from `PassengerId` and analyze whether groups (families/teams) tend to survive together.\n",
        "People in the same group often share the same fate. We create a `group_survival_status` feature:\n",
        "- 0.0: Everyone in the group died.\n",
        "- 1.0: Everyone survived.\n",
        "- 0.5: Mixed survival or unknown.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split PassengerId into Group and Position\n",
        "def split_passenger_id(df):\n",
        "    ids = df['PassengerId'].str.split('_', expand=True)\n",
        "    df['Group'] = ids[0].astype(int)\n",
        "    df['Position'] = ids[1].astype(int)\n",
        "    return df\n",
        "\n",
        "train = split_passenger_id(train)\n",
        "test = split_passenger_id(test)\n",
        "\n",
        "# Group statistics: calculate survival rates per group\n",
        "group_stats = train.groupby('Group')['Transported'].agg(['sum', 'count'])\n",
        "group_stats['survived'] = group_stats['sum']\n",
        "group_stats['total'] = group_stats['count']\n",
        "\n",
        "# Create dictionary: Group -> status\n",
        "# 0: all perished, 1: all survived, 0.5: mixed\n",
        "group_status = {}\n",
        "for group, row in group_stats.iterrows():\n",
        "    if row['survived'] == 0:\n",
        "        group_status[group] = 0\n",
        "    elif row['survived'] == row['total']:\n",
        "        group_status[group] = 1\n",
        "    else:\n",
        "        group_status[group] = 0.5\n",
        "\n",
        "# Map back to train and test\n",
        "train['group_survival_status'] = train['Group'].map(group_status)\n",
        "# For test set, new groups get 0.5 (unknown status)\n",
        "test['group_survival_status'] = test['Group'].map(group_status).fillna(0.5)\n",
        "\n",
        "print(\"Group Stats Head:\")\n",
        "print(group_stats.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Preprocessing and Normalization\n",
        "Scale numerical features like `Age`, `RoomService`, etc. to the range [0, 1] using MinMaxScaler. This helps the neural network converge faster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "cols_to_normalize = ['Age', 'Cabin', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
        "\n",
        "# Normalize train\n",
        "train[cols_to_normalize] = scaler.fit_transform(train[cols_to_normalize])\n",
        "# Transform test with the same scaler\n",
        "test[cols_to_normalize] = scaler.transform(test[cols_to_normalize])\n",
        "\n",
        "# Prepare data for model\n",
        "features = ['HomePlanet', 'group_survival_status', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
        "X_train = train[features]\n",
        "y_train = train['Transported'].astype(int)\n",
        "X_test = test[features]\n",
        "\n",
        "print(\"Training Data Shape:\", X_train.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Model Training\n",
        "We define a sequential neural network with:\n",
        "- Input layer matching the number of features.\n",
        "- Two dense hidden layers with ReLU activation.\n",
        "- An output layer with Sigmoid activation for binary classification.\n",
        "\n",
        "We use **EarlyStopping** to prevent overfitting and **TensorBoard** for monitoring training progress.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Architecture\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(len(features),)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "tensorboard_cb = TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    X_train, y_train, \n",
        "    epochs=12, \n",
        "    batch_size=64, \n",
        "    validation_split=0.2, \n",
        "    verbose=1, \n",
        "    callbacks=[early_stop, tensorboard_cb]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Evaluation and Visualization\n",
        "Plot the training and validation loss/accuracy to assess model performance.\n",
        "We also visualize the age distribution and passenger counts per planet.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot Training History\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Over Epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['accuracy'], label='Accuracy', color='green')\n",
        "plt.plot(history.history['val_accuracy'], label='Val Accuracy', color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Histogram of Passenger Age\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.hist(train['Age'], bins=30, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Age (Normalized)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Passenger Age Distribution')\n",
        "plt.show()\n",
        "\n",
        "# Bar Chart of Passengers by HomePlanet\n",
        "if 'HomePlanet' in train.columns:\n",
        "    plt.figure(figsize=(6,4))\n",
        "    train['HomePlanet'].value_counts().plot(kind='bar', color='orange')\n",
        "    plt.xlabel('HomePlanet (Encoded)')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Passengers by HomePlanet')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Prediction and Submission\n",
        "Generate predictions for the test set and save them to a CSV file for Kaggle submission.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "preds = model.predict(X_test)\n",
        "preds = (preds > 0.5).astype(bool)\n",
        "\n",
        "# Prepare submission dataframe\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test['PassengerId'],\n",
        "    'Transported': preds.flatten()\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file saved as 'submission.csv'\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "spaceTitanicByZ",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 3220602,
          "sourceId": 34377,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31089,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
